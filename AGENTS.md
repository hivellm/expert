<!-- RULEBOOK:START -->
# Project Rules

Generated by @hivellm/rulebook
Generated at: 2025-11-04T05:40:24.440Z

## Core Rules

This project uses @hivellm/rulebook standards.

**CRITICAL RULES:**
1. Always reference @AGENTS.md before coding
2. Write tests first (95%+ coverage required)
3. Run quality checks before committing:
   - Type check / Compiler check
   - Linter (no warnings allowed)
   - All tests (100% pass rate)
   - Coverage check
4. Update docs/ when implementing features
5. Follow strict documentation structure
6. **NEVER create scripts to generate training data** - When asked to enrich or generate datasets, always write the data directly to JSONL files manually. Do NOT create Python scripts for data generation.

## Detailed Rules

For comprehensive rules, see the corresponding files in `/rulebook/`:

- `/rulebook/QUALITY_ENFORCEMENT.md` - Quality enforcement rules
- `/rulebook/GIT.md` - Git workflow rules

Language-specific rules are in `/rulebook/`.
Module-specific patterns are in `/rulebook/`.

When in doubt, ask to review @AGENTS.md first.

<!-- RULEBOOK:END -->

## Language-Specific Rules

The following languages are configured for this project. For detailed rules, see the corresponding files in `/rulebook/`:

### Python Development Rules

For comprehensive Python-specific guidelines, see `/rulebook/PYTHON.md`

Quick reference:
- Type safety and strict mode
- Code quality standards
- Testing requirements (95%+ coverage)
- Package management
- Error handling patterns

**CRITICAL: Always use CLI venv_windows for Python tests**

When running Python scripts for testing, evaluation, or model inference, **ALWAYS** use the `venv_windows` from the CLI:

```powershell
# ✅ CORRECT - Use CLI venv_windows
F:/Node/hivellm/expert/cli/venv_windows/Scripts/python.exe scripts/compare_checkpoints.py

# ❌ WRONG - Never use system Python or other venvs
python scripts/compare_checkpoints.py  # Missing CUDA support!
```

**Why?**
- The CLI venv_windows has CUDA-enabled PyTorch
- Includes all required dependencies (transformers, peft, unsloth, etc.)
- Configured with correct CUDA paths and libraries
- Ensures consistent environment across all tests

**When to use:**
- Running checkpoint comparison scripts
- Testing model inference
- Evaluating expert quality
- Any Python script that requires CUDA or model loading

### Rust Development Rules

For comprehensive Rust-specific guidelines, see `/rulebook/RUST.md`

Quick reference:
- Type safety and strict mode
- Code quality standards
- Testing requirements (95%+ coverage)
- Package management
- Error handling patterns

**Usage**: When working with language-specific code, reference the corresponding `/rulebook/[LANGUAGE].md` file for detailed guidelines.

## Expert CLI Build Rules

**CRITICAL: Always build CLI with CUDA support**

When building the expert-cli (located in `/cli/`), **ALWAYS** use the `build-cuda.ps1` script:

```powershell
# ✅ CORRECT - Build with CUDA support
cd F:/Node/hivellm/expert/cli
./scripts/build-cuda.ps1

# ❌ WRONG - Never use cargo build directly
cargo build --release  # Missing CUDA support!
```

**Why?**
- The CLI requires CUDA support for GPU inference
- `build-cuda.ps1` sets up the correct Visual Studio environment
- Configures CUDA_PATH and CUDNN_PATH correctly
- Enables the `cuda` feature flag automatically

**Output**: `./target/release/expert-cli.exe`

**When to rebuild:**
- After modifying any CLI source code (`src/`)
- After changing inference engine (`src/inference/`)
- After updating Cargo.toml dependencies

## PyTorch/Transformers Specific Rules

**CRITICAL: Always use `dtype` instead of `torch_dtype`**

When loading models with `AutoModelForCausalLM.from_pretrained()` or similar transformers functions:

```python
# ❌ WRONG (deprecated)
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    torch_dtype=torch.bfloat16,  # DEPRECATED!
    device_map="cuda"
)

# ✅ CORRECT
model = AutoModelForCausalLM.from_pretrained(
    model_path,
    dtype=torch.bfloat16,  # Use dtype
    device_map="cuda"
)
```

**Reason**: `torch_dtype` is deprecated in transformers 4.x+. Always use `dtype` to avoid warnings.

**When creating this code pattern:**
- Search for `torch_dtype` in existing code
- Replace with `dtype`
- Apply to all model loading calls

## Module-Specific Instructions

The following modules are configured for this project. For detailed instructions, see the corresponding files in `/rulebook/`:

### Agent automation Instructions

For comprehensive Agent automation-specific instructions, see `/rulebook/AGENT_AUTOMATION.md`

Quick reference:
- Module-specific instructions
- Usage guidelines
- Integration patterns

### Openspec Instructions

For comprehensive Openspec-specific instructions, see `/rulebook/OPENSPEC.md`

Quick reference:
- Module-specific instructions
- Usage guidelines
- Integration patterns

### Context7 Instructions

For comprehensive Context7-specific instructions, see `/rulebook/CONTEXT7.md`

Quick reference:
- Module-specific instructions
- Usage guidelines
- Integration patterns

**Usage**: When working with module-specific features, reference the corresponding `/rulebook/[MODULE].md` file for detailed instructions.
