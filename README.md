# Expert System

> Dynamic expert composition system for fast, specialized inference on domestic GPUs

## Overview

The Expert System is a novel architecture for running specialized AI inference on consumer hardware by dynamically composing lightweight expert adapters on top of a compact base model. Instead of deploying massive, generalized models, this system loads task-specific "experts" on-demand, enabling:

- **Fast inference** on 8-16GB GPUs (RTX 3060-4090 range)
- **High specialization** through focused expert training
- **Marketplace ecosystem** for sharing and discovering experts
- **Long context support** (120k-200k tokens)
- **Parallel multi-agent** execution

## Motivation

Modern LLMs face a fundamental trade-off:
- **Large models** (70B+): Expensive, slow, require datacenter GPUs
- **Small models** (<7B): Fast and cheap, but lack depth in specialized tasks

**Expert System solves this** by keeping a tiny base model (Qwen3-0.6B, ~0.5GB VRAM) permanently loaded and dynamically attaching up to 10 specialist adapters (5-80MB each) per inference. Each expert is trained on synthetic data generated by premium LLMs, democratizing access to specialized AI.

### Key Innovation

Experts are **never merged** into the base model. They exist as runtime-composable sub-tensors (LoRA, DoRA, IAÂ³, soft-prompts) that can be loaded/unloaded in milliseconds. This enables:

1. **Rapid switching** between specializations
2. **Low VRAM footprint** (only load what you need)
3. **Easy distribution** via marketplace
4. **Parallel execution** of different expert combinations

## Not Mixture-of-Experts (MoE)

**Important:** This is NOT a Mixture-of-Experts (MoE) architecture. The approaches are fundamentally different:

### MoE Architecture (e.g., Mixtral, GPT-4)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚         Monolithic Model (~100GB+)          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚  â”‚Expert 1â”‚  â”‚Expert 2â”‚  â”‚Expert Nâ”‚        â”‚
â”‚  â”‚ (MLP)  â”‚  â”‚ (MLP)  â”‚  â”‚ (MLP)  â”‚        â”‚
â”‚  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜  â””â”€â”€â”€â”¬â”€â”€â”€â”€â”˜        â”‚
â”‚      â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”˜             â”‚
â”‚          â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”              â”‚
â”‚          â”‚  Gating Network  â”‚              â”‚
â”‚          â”‚ (Learnable, 1-3) â”‚              â”‚
â”‚          â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Characteristics**:
- âœ… Experts are **neural network layers** (MLPs) inside the model
- âœ… All experts **trained together** as single model
- âœ… Gating network **learned during training** (automatic selection)
- âœ… Typically activates **1-3 experts per token** (sparse routing)
- âŒ **Cannot add new experts** after training
- âŒ **Cannot distribute experts separately** (monolithic)
- âŒ **Requires 100GB+** to load entire model (all experts in memory)
- âŒ **Expensive to train** (~$1M+ for Mixtral-scale)

### HiveLLM Expert System (This Project)
```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚    Base Model (Qwen3-0.6B, ~0.5GB)        â”‚
â”‚         Always in VRAM                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
               â”‚ Runtime composition
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚          â”‚          â”‚
â”Œâ”€â”€â”€â–¼â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â” â”Œâ”€â”€â–¼â”€â”€â”€â”€â”€â”
â”‚SQL.exp â”‚ â”‚Neo4j   â”‚ â”‚Python  â”‚  â† Loaded on-demand
â”‚(LoRA)  â”‚ â”‚(DoRA)  â”‚ â”‚(IAÂ³)   â”‚    (1-10ms each)
â”‚25 MB   â”‚ â”‚30 MB   â”‚ â”‚15 MB   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
     â–²          â–²          â–²
     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         Router (CPU)
    Heuristic-based selection
```

**Characteristics**:
- âœ… Experts are **external adapters** (LoRA/DoRA/IAÂ³ weights)
- âœ… Each expert **trained independently** (separate datasets)
- âœ… Router is **heuristic/rule-based** (keywords, embeddings, user choice)
- âœ… Activates **1-10 experts per query** (loaded as needed)
- âœ… **Can add new experts anytime** (train and deploy separately)
- âœ… **Distributable via Git** (each expert is ~5-80MB file)
- âœ… **Only loads ~1GB total** (base + needed adapters)
- âœ… **Cheap to train** ($15-50 per expert with synthetic data)

### Key Differences

| Aspect | MoE (Mixtral, GPT-4) | HiveLLM Expert System |
|--------|---------------------|----------------------|
| **Expert Type** | Neural layers (MLPs) | PEFT adapters (LoRA/DoRA) |
| **Training** | All experts trained together | Each expert trained separately |
| **Selection** | Learned gating network | Heuristic router (CPU) |
| **Granularity** | Per-token routing | Per-query routing |
| **Extensibility** | Fixed at training time | Add experts anytime |
| **Distribution** | Monolithic model | Individual .expert files |
| **Memory** | Load entire model (~100GB+) | Load base + needed experts (~1GB) |
| **Cost** | $1M+ to train | $15-50 per expert |
| **Use Case** | General-purpose LLM | Specialized task composition |

### When to Use Each

**Use MoE (Mixtral, GPT-4) when:**
- Need general-purpose LLM for all tasks
- Have datacenter GPU (100GB+ VRAM)
- Budget for expensive training/inference
- Want automatic expert selection per-token

**Use HiveLLM Expert System when:**
- Need specialized AI for specific domains
- Have consumer GPU (8-16GB VRAM)
- Want to train/distribute experts independently
- Need explicit control over expert selection
- Want marketplace ecosystem for sharing experts

## Use Cases

### Document Classification
```
Input: "Classify this JSON document about Neo4j schema"
Router selects: [json-parser.expert, english.expert, neo4j-schema.expert, classifier.expert]
â†’ Fast, accurate classification with ~1GB total VRAM
```

### Multi-language Support
```
Input: "Translate technical docs from PT-BR to EN"
Router selects: [portuguese.expert, english.expert, technical-writing.expert]
â†’ Specialized translation without 100B parameter model
```

### Code Generation
```
Input: "Generate Rust async handler for HTTP endpoints"
Router selects: [rust.expert, async-patterns.expert, http-api.expert]
â†’ Domain-specific code without full ChatGPT
```

## Architecture at a Glance

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      User Prompt                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Router/Reasoning (CPU)     â”‚â—„â”€â”€â”€ Heuristics + Embeddings
        â”‚   - Classify task            â”‚     + Mini-policy
        â”‚   - Select up to 10 experts  â”‚
        â”‚   - Tune parameters          â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚   Expert Loader (SSDâ†’VRAM)   â”‚
        â”‚   - Hot-load adapters        â”‚
        â”‚   - ~1-10ms per expert       â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â”‚  Inference Runtime (GPU)     â”‚
        â”‚  Base: Qwen3-0.6B (INT4)     â”‚â—„â”€â”€â”€ Paged KV cache
        â”‚  + Experts (LoRA/IAÂ³/DoRA)   â”‚     CUDA streams
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
                       â–¼
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚ Output â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## Core Components

1. **Base Model (MB)**: Qwen3-0.6B quantized (INT4/INT8), ~0.3-0.6GB VRAM
2. **Experts (EXPs)**: Lightweight adapters packaged as `.expert` files (5-80MB each)
3. **Router (RG)**: CPU-based selection engine using heuristics + embeddings + mini-policy
4. **Inference Runtime (RI)**: GPU engine with hot-swap adapter support and paged KV cache
5. **Marketplace**: Catalog of signed, versioned experts with compatibility checking
6. **Orchestrator**: Multi-agent job queue with telemetry and preemption

## System Requirements

### Hardware
- **GPU**: NVIDIA RTX 3060 (8GB VRAM) or better
  - **Recommended**: RTX 4090 (24GB VRAM)
  - **CUDA**: 12.1+ (tested with 12.1)
- **RAM**: 16GB+ system memory
- **Storage**: 50GB+ SSD (for base model + experts)

### Software
- **Python**: 3.12.x (Python 3.13 not supported by PyTorch yet)
- **PyTorch**: 2.5.1+cu121 (tested and validated)
- **CUDA Toolkit**: 12.1+ (12.1 tested on Windows)
- **OS**: Windows 10/11, Linux (Ubuntu 22.04+)

### Windows CUDA Build Requirements
For GPU acceleration, you need:
1. **Visual Studio 2022 Community** (with C++ Desktop Development workload)
   - Download: https://visualstudio.microsoft.com/downloads/
   - During install, select "Desktop development with C++"
   - Includes `cl.exe` compiler required by CUDA
2. **CUDA Toolkit 12.1**
   - Download: https://developer.nvidia.com/cuda-downloads
   - Install to: `C:\Program Files\NVIDIA GPU Computing Toolkit\CUDA\v12.1`

**Note**: cuDNN is not required for training pipeline (handled by PyTorch). Only needed for custom CUDA kernels.

## Quick Start

### Windows Setup

**Prerequisites**: 
- Python 3.12 from [python.org](https://www.python.org/downloads/release/python-31211/) (NOT Microsoft Store version)
- Rust toolchain from [rustup.rs](https://rustup.rs/)

```powershell
# Clone repository
git clone https://github.com/hivellm/expert-system
cd expert-system/expert

# Build CLI (CPU version)
cd cli
cargo build --release

# Build CLI with CUDA support (RTX 3060+)
# Requires Visual Studio 2022 + CUDA Toolkit
.\scripts\build-cuda.ps1

# Install expert from package
.\target\release\expert-cli.exe install experts\expert-sql\expert-sql-qwen3-0-6b.v0.2.0.expert
.\target\release\expert-cli.exe install experts\expert-neo4j\expert-neo4j-qwen3-0-6b.v0.1.0.expert

# Use expert in one-shot mode (clean output)
.\target\release\expert-cli.exe chat --experts sql --prompt "Find all users older than 25. Schema: users(id,name,age)" --max-tokens 50 --device cuda

# Router automatically uses base model for generic queries
.\target\release\expert-cli.exe chat --experts sql --prompt "What is SQL?" --max-tokens 30 --device cuda
# Output: "SQL is a programming language for managing databases..."

# Router automatically uses expert for domain-specific queries
.\target\release\expert-cli.exe chat --experts sql --prompt "SELECT name FROM users WHERE age > 30" --max-tokens 30 --device cuda
# Output: "SELECT name FROM users WHERE age > 30;"

# Use expert with debug mode (shows routing decision)
.\target\release\expert-cli.exe chat --experts neo4j --prompt "Find actors" --max-tokens 50 --device cuda --debug

# List installed experts
.\target\release\expert-cli.exe list
```

**CUDA Build Notes:**
- CPU build: `cargo build --release` (works everywhere)
- CUDA build: `.\scripts\build-cuda.ps1` (requires VS2022 + CUDA Toolkit 12.1+)
- Script auto-configures Visual Studio environment and CUDA paths
- First CUDA build takes ~10-15 minutes (compiles CUDA kernels)
- RTX 4090: 10-50x faster inference than CPU
- No cuDNN required for inference (only for training)

### Linux Setup

```bash
# Clone repository
git clone https://github.com/hivellm/expert-system
cd expert-system/expert

# Build CLI
cd cli
cargo build --release

# Train first expert (e.g., JSON parser)
cd ../experts/expert-json-parser
expert-cli train \
  --manifest manifest.json \
  --dataset datasets/json_8k.jsonl \
  --output weights \
  --device auto
```

**ðŸš€ Ready for more?** See **[QUICKSTART.md](QUICKSTART.md)** for a practical 4-week plan to:
1. Train your first 6 experts (English, JSON, Neo4j, Python, Rust, Classifier)
2. Build a simple Python CLI to test them
3. Benchmark performance and validate the architecture
4. **Cost**: $15-25 (synthetic data + GPU time)

This practical approach lets you validate the concept before building the full Rust runtime.

---

### Installation (Future - Full Release)

```bash
# Install expert system
curl -sSL https://expert.hivellm.dev/install.sh | bash

# Download base model
expert-cli model download qwen3-0.6b-int4

# Install some experts from marketplace
expert-cli expert install english-basic@1.0.0
expert-cli expert install json-parser@2.1.3
expert-cli expert install neo4j-schema@1.5.0
```

### Basic Usage (Future)

```typescript
import { ExpertEngine } from '@hivellm/expert';

const engine = new ExpertEngine({
  baseModel: 'qwen3-0.6b-int4',
  contextSize: 131072
});

// Automatic expert selection
const result = await engine.infer({
  prompt: "Parse this JSON and extract Neo4j node types",
  body: jsonDocument
});

console.log(result.output);
console.log(`Used experts: ${result.expertsUsed.join(', ')}`);
console.log(`Latency: ${result.latencyMs}ms`);
```

### Training Custom Expert

```bash
# Train expert using CLI
expert-cli train \
  --manifest manifest.json \
  --dataset datasets/my_dataset.jsonl \
  --output weights \
  --device auto

# Validate trained expert
expert-cli validate --expert weights/adapter

# Package and sign
expert-cli package \
  --manifest manifest.json \
  --weights weights/adapter \
  --output weights/my-expert.v1.0.0.expert

expert-cli sign --expert weights/my-expert.v1.0.0.expert
```

**CLI Commands:**
- `train` - Train expert from manifest and dataset
- `validate` - Validate trained expert on test set
- `package` - Package expert into .expert file
- `sign` - Cryptographically sign expert package
- `install` - Install expert from Git repository
- `list` - List installed experts

## Key Features

### ðŸš€ Dynamic Composition
Load up to 10 experts in ~1-10ms. No model merging, no restart required.

### ðŸ’¾ Low VRAM
Base model + 10 experts typically fit in 8-16GB VRAM. Runs on consumer GPUs.

### ðŸ“ Long Context
120k-200k token support via RoPE scaling (NTK/YaRN) and paged attention.

### ðŸ§¬ Synthetic Data
Generate training datasets using premium LLMs (DeepSeek, Claude, GPT-4) instead of manual curation.

### ðŸŒ Git-Based Distribution
**No NPM! No PyPI!** Experts distributed via Git repositories. Fork, modify, and share your own experts instantly.

```bash
# Install from any Git repository
expert-cli install https://github.com/hivellm/expert-json-parser

# Or from your own repo
expert-cli install https://github.com/yourname/my-expert
```

### ðŸ”’ Decentralized Marketplace
Marketplace is a Git index, not a centralized registry. No approval process, no gatekeepers.

### âš¡ Parallel Execution
Multi-agent orchestrator enables concurrent inference with different expert combinations.

### ðŸŽ¯ Specialization
Each expert focuses on narrow domain (JSON parsing, language, tech stack) for higher accuracy than generalist models.

### ðŸ§­ Intelligent Router (NEW!)
**Preserves base model generality while enabling expert specialization**

```powershell
# Generic query â†’ Base model (no expert)
expert-cli chat --experts neo4j --prompt "What is the capital of France?"
â†’ Output: "Paris"  # Uses base model knowledge

# Specialized query â†’ Expert (automatic)
expert-cli chat --experts neo4j --prompt "Find all people older than 30"
â†’ Output: "MATCH (p:Person) WHERE p.age > 30 RETURN p"  # Uses Neo4j expert
```

**How it works:**
- **Keyword detection**: Manifest defines domain keywords (`sql`, `cypher`, `graph`)
- **Generic query detection**: Patterns like "what is", "explain", "how to" trigger base model
- **Automatic selection**: Router picks expert vs base based on query content
- **Clean output**: Removes ChatML artifacts (`<|end|>`, `<|endoftext|>`)
- **Multi-expert support**: Intelligently selects best expert when multiple loaded

**Configuration in manifest.json:**
```json
{
  "routing": {
    "keywords": ["sql", "database", "query", "select"],
    "exclude_keywords": ["what is", "explain", "meaning"],
    "priority": 0.85
  }
}
```

**Validation (100% test pass rate):**
- âœ… Generic queries preserve generalist capabilities
- âœ… Domain queries activate correct expert
- âœ… Multi-expert routing works correctly
- âœ… Zero overhead (<0.5ms router latency)

## Performance Targets

| Metric | Target | Notes |
|--------|--------|-------|
| Base VRAM | 0.3-0.6 GB | Qwen3-0.6B INT4/INT8 |
| Expert VRAM | 5-80 MB each | Depends on LoRA rank |
| Total VRAM | 8-16 GB | Base + 10 experts |
| Hot-load latency | 1-10 ms | Pre-mapped weights |
| Cold-load latency | 50-200 ms | SSD read + decompress |
| Context window | 120k-200k tokens | Via YaRN/NTK scaling |
| Inference speed | ~50-100 tok/s | GPU-dependent, batched |

## Project Status

**Current Phase**: Rust Runtime Implementation (active)  
**Completed**:
- âœ… Complete documentation suite
- âœ… Expert training CLI (Python + PyTorch/PEFT/TRL)
- âœ… Windows setup automation (CUDA 12.1 + PyTorch 2.5.1)
- âœ… Training optimizations (LLaMA-Factory/Unsloth best practices)
- âœ… Dataset preprocessing pipeline (validation, deduplication, SQL fixing)
- âœ… Synthetic data generation pipeline
- âœ… **Expert-SQL v0.2.0** trained and packaged (99k examples, 12.4/10 quality)
- âœ… **Expert-Neo4j v0.1.0** trained and packaged (29k examples, 9.13/10 quality)
- âœ… **Rust inference runtime** with Candle + CUDA support
- âœ… **LoRA/DoRA adapter merging** in Rust (168 weights per expert)
- âœ… **Expert packaging** (.expert tar.gz format)
- âœ… **Expert installation** from packages
- âœ… **Registry system** for tracking installed experts
- âœ… **One-shot mode** (--prompt) for scripting
- âœ… **Comprehensive test suite** (5 automated test scripts)

**Validated Experts**:
| Expert | Version | Quality | Status |
|--------|---------|---------|--------|
| expert-sql | v0.2.0 | 12.4/10 | âœ… Production ready |
| expert-neo4j | v0.1.0 | 9.13/10 | âœ… Production ready |
| expert-json | v0.1.0 | TBD | ðŸ”„ Training |

**In Progress**:
- ðŸ”„ Training remaining experts (JSON, Python, Rust, TypeScript)
- ðŸ”„ Multi-expert routing and composition
- ðŸ”„ Performance optimization (caching merged weights)

**Next Milestone**: P1 - Multi-expert orchestration and routing

**Training Improvements Applied**:
- **Dataset quality**: MySQLâ†’PostgreSQL syntax fixes, validation via sqlglot
- **Hyperparameters**: LR 5e-5, temp 0.7, dropout 0.1, warmup 10% (LLaMA-Factory/Unsloth best practices)
- **Checkpointing**: Every 250 steps with eval monitoring and best model selection
- **Memory optimization**: Dataset reduced by 77% (text-only format)
- **Unsloth integration**: Optional 2x speedup via `use_unsloth` flag in manifest
- **Windows compatibility**: torch.compile disabled, triton conflicts resolved

See [STATUS.md](STATUS.md) for detailed tracking.

## Documentation

- [CLI.md](docs/CLI.md) - **Unified CLI commands (dataset, train, package, install)**
- [ARCHITECTURE.md](docs/ARCHITECTURE.md) - Detailed component breakdown
- [EXPERT_FORMAT.md](docs/EXPERT_FORMAT.md) - `.expert` package specification
- [GIT_DISTRIBUTION.md](docs/GIT_DISTRIBUTION.md) - **Git-based expert distribution (no NPM!)**
- [EXECUTION_PIPELINE.md](docs/EXECUTION_PIPELINE.md) - Four-stage inference flow
- [TRAINING_GUIDE.md](docs/TRAINING_GUIDE.md) - How to train custom experts
- [SYNTHETIC_DATA.md](docs/SYNTHETIC_DATA.md) - LLM-based dataset generation
- [ROUTING_REASONING.md](docs/ROUTING_REASONING.md) - Router design and selection logic
- [PERFORMANCE.md](docs/PERFORMANCE.md) - Resource budgeting and optimization
- [ROADMAP.md](ROADMAP.md) - Implementation phases (P0-P6)

## Examples

See [examples/](examples/) directory for:
- Sample manifest.json files
- Expert training scripts
- Synthetic data generation prompts
- API usage patterns

## Technology Stack

**Hybrid Python + Rust architecture:**

### Python (Training & Tooling)
- **PyTorch 2.5.1+cu121** for GPU acceleration (CUDA 12.1)
- **PEFT 0.17+** for LoRA/IAÂ³/DoRA adapters
- **Transformers 4.57+** for model loading and inference
- **TRL 0.7+** for SFTTrainer with sequence packing
- **BitsAndBytes 0.48+** for INT4/INT8 quantization (QLoRA)
- Expert training pipelines and validation
- Synthetic data generation (DeepSeek, Claude, GPT-4 integration)
- **Training optimizations**: LLaMA-Factory/Unsloth-inspired best practices
  - Learning rate: 5e-5 (conservative for small models)
  - Temperature: 0.7 (Qwen official recommendation)
  - Dropout: 0.1, Warmup: 10% (regularization)
  - Checkpointing: Every 250 steps with eval monitoring

**Why Python:** Unbeatable ML ecosystem, rapid iteration, proven CUDA support

**Current Status**: âœ… Fully functional training pipeline on Windows + Linux  
**Unsloth Support**: âœ… Optional 2x speedup (set `use_unsloth: true` in manifest)  
**Performance**: 
- Without Unsloth: ~70-80% baseline speed (stable)
- With Unsloth: ~2x faster + 70% less VRAM (requires compatible dependencies)

### Rust (Runtime & Production)
- **Candle** (HuggingFace) for inference engine
- **SafeTensors** for weight loading and adapter merging
- âœ… **LoRA/DoRA adapter merging** implemented (W' = W + scale Ã— B Ã— A)
- âœ… **Expert hot-loading** from .expert packages
- âœ… **Registry system** for installed experts
- âœ… **CUDA acceleration** with BF16/FP32 support
- âœ… **One-shot inference** mode for scripting
- ðŸ”„ Paged KV cache implementation
- ðŸ”„ Marketplace CLI and verification
- ðŸ”„ gRPC/HTTP API + Node/Python bindings

**Why Rust:** Performance, single binary deployment, memory safety, no GC pauses

**Current Status**: âœ… Functional inference runtime with adapter merging  
**Performance**:
- Adapter merging: ~2-3s first load (merge + CUDA transfer)
- Inference: ~100-150ms per query (RTX 4090)
- Multiple experts: Loads first expert adapter, others TODO

### Tested Hardware
- âœ… **NVIDIA RTX 4090** (24GB VRAM) - Full performance, tested
- âœ… **NVIDIA RTX 3060** (8GB VRAM) - Minimum viable
- âœ… **CUDA 12.1** - Fully functional with PyTorch 2.5.1
- âœ… **Windows 11** + **Ubuntu 24.04** (WSL) - Both validated

See [ARCHITECTURE.md](docs/ARCHITECTURE.md#technology-stack) for detailed breakdown.

## Contributing

This is currently a design/architecture phase. Implementation contributions welcome once P0 milestone begins.

## License

TBD (likely Apache-2.0 or MIT)

## Credits

Architecture influenced by:
- LoRA/QLoRA research (Hu et al., Microsoft)
- vLLM paged attention (Berkeley)
- LLaMA.cpp quantization techniques
- OpenAI's GPT-4 synthetic data strategies
- Constitutional AI (Anthropic)

