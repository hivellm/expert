# Proposal: Expert Inference SDKs

**Change ID**: `add-inference-sdks`  
**Status**: Draft  
**Created**: 2025-11-06  
**Author**: HiveLLM Team

## Overview

Create official SDKs for loading and running Expert inference in Python and Rust, providing a simple, ergonomic API for integrating trained experts into applications.

## Motivation

### Current State (Problems)

**Manual Integration Required**:
- Users must manually load base models, adapters, tokenizers
- Complex setup for PEFT models (LoRA/DoRA)
- Different code for each programming language
- No standardized way to load `.expert` packages
- Grammar validation and decoding params are manual

**Example Current Workflow (Python)**:
```python
# Manual, verbose setup
from transformers import AutoModelForCausalLM, AutoTokenizer
from peft import PeftModel
import torch

base_model = AutoModelForCausalLM.from_pretrained(...)
tokenizer = AutoTokenizer.from_pretrained(...)
model = PeftModel.from_pretrained(base_model, adapter_path)

# Manual message formatting
messages = [{"role": "system", "content": "..."}, ...]
text = tokenizer.apply_chat_template(...)
inputs = tokenizer([text], return_tensors="pt").to(model.device)

# Manual generation
outputs = model.generate(**inputs, temperature=0.7, ...)
result = tokenizer.decode(outputs[0][...], skip_special_tokens=True)
```

**No Package Support**:
- `.expert` files must be manually extracted
- Manifest parsing is manual
- No validation of package integrity
- No automatic model discovery

**Inconsistent Experience**:
- Different APIs for Rust vs Python
- No shared conventions
- Difficult to port code between languages

### Desired State (Solution)

**Simple, Unified SDK**:

```python
# Python SDK - Simple and clean
from hivellm_expert import Expert

# Single expert
expert = Expert.load("expert-sql-qwen3-0-6b.v0.2.0.expert")
result = expert.generate("List all users who registered in 2024")
print(result.text)

# Multiple experts
from hivellm_expert import ExpertManager

manager = ExpertManager()
manager.load_expert("sql", "expert-sql.v0.2.0.expert")
manager.load_expert("cypher", "expert-neo4j.v0.1.0.expert")

# Route queries to appropriate expert
sql_result = manager.generate("sql", "List users from 2024")
cypher_result = manager.generate("cypher", "Find related movies")
```

```rust
// Rust SDK - Ergonomic and type-safe
use hivellm_expert::{Expert, ExpertManager};

// Single expert
let expert = Expert::load("expert-sql-qwen3-0-6b.v0.2.0.expert")?;
let result = expert.generate("List all users who registered in 2024")?;
println!("{}", result.text);

// Multiple experts with efficient memory management
let mut manager = ExpertManager::new();
manager.load_expert("sql", "expert-sql.v0.2.0.expert")?;
manager.load_expert("cypher", "expert-neo4j.v0.1.0.expert")?;

// Automatic base model reuse (both use Qwen3-0.6B)
let sql_result = manager.generate("sql", "List users")?;
let cypher_result = manager.generate("cypher", "Find movies")?;
```

**Key Features**:
- âœ… Automatic model + adapter + tokenizer loading
- âœ… `.expert` package support (extract, validate, load)
- âœ… **Multiple expert loading and management**
- âœ… **Base model sharing** (load once, use with multiple adapters)
- âœ… **Hot-swapping adapters** (switch experts without reloading base)
- âœ… Manifest-driven configuration (decoding params, grammar)
- âœ… ChatML template auto-application
- âœ… Grammar validation (when enabled)
- âœ… Streaming support
- âœ… Batch inference
- âœ… Error handling and validation
- âœ… Cross-platform (Windows, Linux, macOS)

## Impact Analysis

### Benefits

**Developer Experience**:
- ðŸ“ˆ **Faster Integration**: ~30 lines of code â†’ 3 lines
- ðŸ“ˆ **Lower Barrier**: No need to understand PEFT, transformers internals
- ðŸ“ˆ **Consistency**: Same API across Python and Rust
- ðŸ“ˆ **Best Practices**: Built-in optimizations (device auto-detection, memory management)

**Enterprise Adoption**:
- ðŸŽ¯ **Production Ready**: Validated package loading, error handling
- ðŸŽ¯ **Type Safety**: Rust SDK provides compile-time guarantees
- ðŸŽ¯ **Performance**: Optimized for inference (CUDA graphs, batching)
- ðŸŽ¯ **Observability**: Built-in logging, metrics, tracing

**Ecosystem Growth**:
- ðŸš€ **More Applications**: Easier to integrate â†’ more adoption
- ðŸš€ **Community**: Standardized SDK â†’ easier to share examples
- ðŸš€ **Packages**: `.expert` becomes first-class citizen

### Risks & Mitigation

| Risk | Likelihood | Impact | Mitigation |
|------|------------|--------|------------|
| API changes breaking users | Medium | High | Semantic versioning, deprecation warnings |
| Performance overhead | Low | Medium | Benchmark against manual approach, optimize hot paths |
| Dependency bloat | Medium | Low | Keep dependencies minimal, make features opt-in |
| Cross-platform issues | Medium | Medium | CI testing on Windows/Linux/macOS |

## Technical Approach

### Python SDK (`hivellm-expert`)

**Architecture**:
```
hivellm_expert/
â”œâ”€â”€ __init__.py           # Main API
â”œâ”€â”€ expert.py             # Expert class
â”œâ”€â”€ loader.py             # Package loading (.expert files)
â”œâ”€â”€ manifest.py           # Manifest parsing/validation
â”œâ”€â”€ inference.py          # Generation, streaming
â”œâ”€â”€ grammar.py            # Grammar validation (optional)
â””â”€â”€ utils.py              # Helpers
```

**Key Classes**:
- `Expert`: Main interface for loading and inference
- `ExpertManifest`: Manifest representation
- `InferenceResult`: Generation output with metadata
- `ExpertLoader`: Package extraction and validation

**Dependencies**:
- `transformers` (required)
- `peft` (required)
- `torch` (required)
- `jsonschema` (manifest validation)
- `llama-cpp-python` (optional, for grammar)

### Rust SDK (`hivellm-expert`)

**Architecture**:
```
src/
â”œâ”€â”€ lib.rs                # Main API
â”œâ”€â”€ expert.rs             # Expert struct
â”œâ”€â”€ loader.rs             # Package loading
â”œâ”€â”€ manifest.rs           # Manifest types (already exists)
â”œâ”€â”€ inference.rs          # Generation logic
â”œâ”€â”€ grammar.rs            # Grammar validation
â””â”€â”€ error.rs              # Error types
```

**Key Structs**:
- `Expert`: Main interface
- `ExpertManifest`: Manifest representation (reuse from CLI)
- `InferenceConfig`: Generation parameters
- `InferenceResult`: Generation output

**Dependencies**:
- `candle-core` (inference engine)
- `tokenizers` (tokenization)
- `serde` / `serde_json` (manifest)
- `tar` / `flate2` (package extraction)

### Integration with Existing Code

**Reuse**:
- âœ… `expert/cli/src/manifest.rs` â†’ Rust SDK manifest types
- âœ… `expert/schemas/expert-manifest.schema.json` â†’ Python validation
- âœ… `expert/cli/expert_trainer.py` â†’ Inference logic patterns
- âœ… `expert/cli/src/engines/qwen3_engine.rs` â†’ Candle engine

**New**:
- Package extraction and validation
- Simple public API (currently CLI-focused)
- Streaming generators
- Batch inference helpers

## Success Criteria

### Functional

- [ ] Python SDK loads `.expert` packages
- [ ] Rust SDK loads `.expert` packages
- [ ] Both SDKs generate correct outputs (validate against manual approach)
- [ ] Grammar validation works (when enabled)
- [ ] Streaming inference works
- [ ] Batch inference works (multiple prompts)
- [ ] Error handling is comprehensive

### Non-Functional

- [ ] Performance within 5% of manual approach
- [ ] Documentation covers 90% of use cases
- [ ] Test coverage >80%
- [ ] Works on Windows, Linux, macOS
- [ ] Published to PyPI (Python) and crates.io (Rust)

### User Acceptance

- [ ] 3 lines of code for simple inference
- [ ] No manual model/tokenizer loading
- [ ] Clear error messages
- [ ] Migration guide from manual approach
- [ ] Example applications (REST API, CLI, notebook)

## Timeline

**Phase 1: Python SDK (Week 1-2)**
- Core API design
- Package loading
- Basic inference
- Unit tests
- Documentation

**Phase 2: Rust SDK (Week 2-3)**
- Core API design
- Package loading (reuse CLI code)
- Candle-based inference
- Unit tests
- Documentation

**Phase 3: Advanced Features (Week 3-4)**
- Streaming inference (both SDKs)
- Batch inference (both SDKs)
- Grammar validation (both SDKs)
- Performance optimization

**Phase 4: Release (Week 4)**
- Integration tests
- Example applications
- PyPI / crates.io publish
- Announcement & docs

## Multiple Experts Support

### Use Cases

**1. Multi-Domain Applications**:
- Web app supporting SQL + Neo4j + TypeScript queries
- Load all experts at startup
- Route queries based on user intent

**2. Memory-Efficient Deployment**:
- Multiple experts share same base model (Qwen3-0.6B)
- Base model loaded once (~1.2GB)
- Each adapter adds only ~25-50MB

**3. Dynamic Expert Loading**:
- Load experts on-demand based on request
- Unload inactive experts to save memory
- LRU cache for frequently used experts

### Technical Approach

**Base Model Sharing**:
```python
# Inefficient (loads base model 3 times)
sql_expert = Expert.load("sql.expert")      # 1.2GB
cypher_expert = Expert.load("cypher.expert") # 1.2GB
ts_expert = Expert.load("ts.expert")        # 1.2GB
# Total: ~3.6GB

# Efficient (loads base model once)
manager = ExpertManager()
manager.load_expert("sql", "sql.expert")      # 1.2GB + 25MB
manager.load_expert("cypher", "cypher.expert") # +25MB
manager.load_expert("ts", "ts.expert")        # +25MB
# Total: ~1.3GB (73% memory savings!)
```

**Hot-Swapping**:
```python
# Switch adapters without reloading base model
manager.load_expert("sql", "sql.expert")
result1 = manager.generate("sql", "SELECT * FROM users")

manager.load_expert("cypher", "cypher.expert")  # Swap adapter
result2 = manager.generate("cypher", "MATCH (u:User)")
# Base model stays in memory, only adapter swapped (~50ms)
```

**LRU Eviction** (Rust):
```rust
let mut manager = ExpertManager::builder()
    .max_loaded_experts(3)  // Keep max 3 adapters in memory
    .build();

manager.load_expert("sql", "sql.expert")?;
manager.load_expert("cypher", "cypher.expert")?;
manager.load_expert("ts", "ts.expert")?;
manager.load_expert("python", "python.expert")?;  // Evicts least-used (SQL)

// Automatically reloads if accessed
manager.generate("sql", "SELECT ...")?;  // Reloads SQL, evicts cypher
```

## Open Questions

1. **API Design**: Should we support both high-level (`expert.generate()`) and low-level APIs?
2. **Caching**: Should SDKs cache extracted `.expert` packages?
3. **Model Registry**: Should we support remote model loading (HuggingFace Hub)?
4. **Compatibility**: How to handle breaking changes in manifest format?
5. **Versioning**: Should SDK version match expert schema version?
6. **Multi-Expert Routing**: Should SDK provide automatic routing (keyword-based) or leave to user?
7. **Mixed Base Models**: How to handle experts with different base models (Qwen3 vs Llama)?

## Alternatives Considered

### Alternative 1: CLI-Only Approach
**Decision**: Rejected  
**Reason**: CLI is great for testing but not suitable for programmatic integration. Libraries are needed.

### Alternative 2: Wrapper Script Approach
**Decision**: Rejected  
**Reason**: Fragile, hard to maintain, no type safety, poor DX.

### Alternative 3: Extend Transformers/HuggingFace
**Decision**: Deferred  
**Reason**: Good long-term goal, but requires upstream collaboration. Start with standalone SDK first.

## References

- Expert manifest schema: `expert/schemas/expert-manifest.schema.json`
- CLI implementation: `expert/cli/`
- Qwen3 engine: `expert/cli/src/engines/qwen3_engine.rs`
- PEFT documentation: https://huggingface.co/docs/peft
- Candle docs: https://github.com/huggingface/candle

